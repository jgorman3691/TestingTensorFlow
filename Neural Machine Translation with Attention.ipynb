{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jedgo\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\client\\session.py:1761: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import preprocessing\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_zip = tf.keras.utils.get_file(\n",
    "    'spa-eng.zip', origin = 'http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip', \n",
    "    extract=True\n",
    ")\n",
    "path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "    \n",
    "    w = re.sub(r\"([?.!,¿¡])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "    \n",
    "    w = re.sub(r\"[^a-zA-Z0-9.,!¿¡]\", \" \", w)\n",
    "    \n",
    "    w = w.strip()\n",
    "    \n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> may i borrow this book <end>\n",
      "b'<start> \\xc2\\xbf puedo tomar prestado este libro <end>'\n"
     ]
    }
   ],
   "source": [
    "en_sentence = u\"May I borrow this book?\"\n",
    "sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n",
    "\n",
    "print(preprocess_sentence(en_sentence))\n",
    "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(path, num_examples):\n",
    "    lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "    \n",
    "    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')] for l in lines[:num_examples]]\n",
    "    \n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
      "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
     ]
    }
   ],
   "source": [
    "en, sp = create_dataset(path_to_file, None)\n",
    "print(en[-1])\n",
    "print(sp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    \n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    \n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_examples=None):\n",
    "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
    "    \n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = 50000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
    "\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 40000 10000 10000\n"
     ]
    }
   ],
   "source": [
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if (t != 0):\n",
    "            print(\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language: Index to Word Mapping\n",
      "1 ----> <start>\n",
      "373 ----> tenes\n",
      "4386 ----> lindos\n",
      "271 ----> ojos\n",
      "3 ----> .\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language: Index to Word Mapping\n",
      "1 ----> <start>\n",
      "5 ----> you\n",
      "25 ----> have\n",
      "901 ----> cute\n",
      "330 ----> eyes\n",
      "3 ----> .\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Language: Index to Word Mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print()\n",
    "print(\"Target Language: Index to Word Mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 16]), TensorShape([64, 12]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gru(units):\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        return tf.keras.layers.GRU(units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
    "    else:\n",
    "        return tf.keras.layers.GRU(units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = gru(self.enc_units)\n",
    "        \n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 16, 1024)\n",
      "Encoder hidden state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# Sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print('Encoder hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden_size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden_size)\n",
    "        # values shape == (batch_size, max_len, hidden_size)\n",
    "        # We're doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # We get 1 at the last axis because we're applying the score to self.V\n",
    "        # Shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "    \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)l\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True,\n",
    "                                      recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # Passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output decoder shape: (batch size, vocab size) (64, 6888)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)), sample_hidden, sample_output)\n",
    "\n",
    "print(\"Output decoder shape: (batch size, vocab size) {}\".format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        \n",
    "        dec_hidden = enc_hidden\n",
    "        \n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "        \n",
    "        # Teacher forcing, feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # Passing enc-output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            \n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            \n",
    "            # Using Teacher Forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "            \n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    \n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    \n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    \n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    " \n",
    "    return batch_loss      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 4.8789\n",
      "Epoch 1 Batch 100 Loss 2.2300\n",
      "Epoch 1 Batch 200 Loss 2.0340\n",
      "Epoch 1 Batch 300 Loss 1.9298\n",
      "Epoch 1 Batch 400 Loss 1.6715\n",
      "Epoch 1 Batch 500 Loss 1.8108\n",
      "Epoch 1 Batch 600 Loss 1.5836\n",
      "Epoch 1 Loss 1.9515\n",
      "Time taken for 1 epoch: 44.488365173339844 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 1.3786\n",
      "Epoch 2 Batch 100 Loss 1.3477\n",
      "Epoch 2 Batch 200 Loss 1.1445\n",
      "Epoch 2 Batch 300 Loss 1.1491\n",
      "Epoch 2 Batch 400 Loss 1.1911\n",
      "Epoch 2 Batch 500 Loss 0.8970\n",
      "Epoch 2 Batch 600 Loss 0.8309\n",
      "Epoch 2 Loss 1.1634\n",
      "Time taken for 1 epoch: 39.17271161079407 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.7507\n",
      "Epoch 3 Batch 100 Loss 0.6981\n",
      "Epoch 3 Batch 200 Loss 0.7351\n",
      "Epoch 3 Batch 300 Loss 0.8520\n",
      "Epoch 3 Batch 400 Loss 0.7356\n",
      "Epoch 3 Batch 500 Loss 0.7408\n",
      "Epoch 3 Batch 600 Loss 0.5844\n",
      "Epoch 3 Loss 0.7051\n",
      "Time taken for 1 epoch: 37.54280066490173 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.4111\n",
      "Epoch 4 Batch 100 Loss 0.4491\n",
      "Epoch 4 Batch 200 Loss 0.4701\n",
      "Epoch 4 Batch 300 Loss 0.3973\n",
      "Epoch 4 Batch 400 Loss 0.4335\n",
      "Epoch 4 Batch 500 Loss 0.4634\n",
      "Epoch 4 Batch 600 Loss 0.4132\n",
      "Epoch 4 Loss 0.4508\n",
      "Time taken for 1 epoch: 40.348212242126465 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 0.2586\n",
      "Epoch 5 Batch 100 Loss 0.2117\n",
      "Epoch 5 Batch 200 Loss 0.2530\n",
      "Epoch 5 Batch 300 Loss 0.2726\n",
      "Epoch 5 Batch 400 Loss 0.2747\n",
      "Epoch 5 Batch 500 Loss 0.3431\n",
      "Epoch 5 Batch 600 Loss 0.3187\n",
      "Epoch 5 Loss 0.2994\n",
      "Time taken for 1 epoch: 39.98468017578125 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 0.1845\n",
      "Epoch 6 Batch 100 Loss 0.1665\n",
      "Epoch 6 Batch 200 Loss 0.1830\n",
      "Epoch 6 Batch 300 Loss 0.2305\n",
      "Epoch 6 Batch 400 Loss 0.2359\n",
      "Epoch 6 Batch 500 Loss 0.2655\n",
      "Epoch 6 Batch 600 Loss 0.2542\n",
      "Epoch 6 Loss 0.2096\n",
      "Time taken for 1 epoch: 41.70249557495117 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 0.1224\n",
      "Epoch 7 Batch 100 Loss 0.1163\n",
      "Epoch 7 Batch 200 Loss 0.1523\n",
      "Epoch 7 Batch 300 Loss 0.1615\n",
      "Epoch 7 Batch 400 Loss 0.1500\n",
      "Epoch 7 Batch 500 Loss 0.1306\n",
      "Epoch 7 Batch 600 Loss 0.1881\n",
      "Epoch 7 Loss 0.1539\n",
      "Time taken for 1 epoch: 40.190857887268066 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 0.0893\n",
      "Epoch 8 Batch 100 Loss 0.0993\n",
      "Epoch 8 Batch 200 Loss 0.1046\n",
      "Epoch 8 Batch 300 Loss 0.1127\n",
      "Epoch 8 Batch 400 Loss 0.1227\n",
      "Epoch 8 Batch 500 Loss 0.1360\n",
      "Epoch 8 Batch 600 Loss 0.1014\n",
      "Epoch 8 Loss 0.1180\n",
      "Time taken for 1 epoch: 41.269659757614136 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 0.1029\n",
      "Epoch 9 Batch 100 Loss 0.0638\n",
      "Epoch 9 Batch 200 Loss 0.1167\n",
      "Epoch 9 Batch 300 Loss 0.0689\n",
      "Epoch 9 Batch 400 Loss 0.0934\n",
      "Epoch 9 Batch 500 Loss 0.1128\n",
      "Epoch 9 Batch 600 Loss 0.0881\n",
      "Epoch 9 Loss 0.0970\n",
      "Time taken for 1 epoch: 39.92996096611023 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 0.1003\n",
      "Epoch 10 Batch 100 Loss 0.0550\n",
      "Epoch 10 Batch 200 Loss 0.0803\n",
      "Epoch 10 Batch 300 Loss 0.0646\n",
      "Epoch 10 Batch 400 Loss 0.1126\n",
      "Epoch 10 Batch 500 Loss 0.1102\n",
      "Epoch 10 Batch 600 Loss 0.0814\n",
      "Epoch 10 Loss 0.0827\n",
      "Time taken for 1 epoch: 41.6262788772583 sec\n",
      "\n",
      "Epoch 11 Batch 0 Loss 0.0502\n",
      "Epoch 11 Batch 100 Loss 0.0803\n",
      "Epoch 11 Batch 200 Loss 0.0469\n",
      "Epoch 11 Batch 300 Loss 0.0750\n",
      "Epoch 11 Batch 400 Loss 0.0589\n",
      "Epoch 11 Batch 500 Loss 0.1005\n",
      "Epoch 11 Batch 600 Loss 0.0597\n",
      "Epoch 11 Loss 0.0712\n",
      "Time taken for 1 epoch: 40.04526376724243 sec\n",
      "\n",
      "Epoch 12 Batch 0 Loss 0.0495\n",
      "Epoch 12 Batch 100 Loss 0.0458\n",
      "Epoch 12 Batch 200 Loss 0.0738\n",
      "Epoch 12 Batch 300 Loss 0.0846\n",
      "Epoch 12 Batch 400 Loss 0.0596\n",
      "Epoch 12 Batch 500 Loss 0.0693\n",
      "Epoch 12 Batch 600 Loss 0.0825\n",
      "Epoch 12 Loss 0.0667\n",
      "Time taken for 1 epoch: 40.774210691452026 sec\n",
      "\n",
      "Epoch 13 Batch 0 Loss 0.0763\n",
      "Epoch 13 Batch 100 Loss 0.0503\n",
      "Epoch 13 Batch 200 Loss 0.0573\n",
      "Epoch 13 Batch 300 Loss 0.0733\n",
      "Epoch 13 Batch 400 Loss 0.0415\n",
      "Epoch 13 Batch 500 Loss 0.1085\n",
      "Epoch 13 Batch 600 Loss 0.0672\n",
      "Epoch 13 Loss 0.0620\n",
      "Time taken for 1 epoch: 38.56475329399109 sec\n",
      "\n",
      "Epoch 14 Batch 0 Loss 0.0619\n",
      "Epoch 14 Batch 100 Loss 0.0369\n",
      "Epoch 14 Batch 200 Loss 0.0497\n",
      "Epoch 14 Batch 300 Loss 0.0457\n",
      "Epoch 14 Batch 400 Loss 0.0530\n",
      "Epoch 14 Batch 500 Loss 0.0548\n",
      "Epoch 14 Batch 600 Loss 0.0787\n",
      "Epoch 14 Loss 0.0581\n",
      "Time taken for 1 epoch: 40.58441996574402 sec\n",
      "\n",
      "Epoch 15 Batch 0 Loss 0.0315\n",
      "Epoch 15 Batch 100 Loss 0.0399\n",
      "Epoch 15 Batch 200 Loss 0.0517\n",
      "Epoch 15 Batch 300 Loss 0.0749\n",
      "Epoch 15 Batch 400 Loss 0.0386\n",
      "Epoch 15 Batch 500 Loss 0.0473\n",
      "Epoch 15 Batch 600 Loss 0.0614\n",
      "Epoch 15 Loss 0.0574\n",
      "Time taken for 1 epoch: 40.08141756057739 sec\n",
      "\n",
      "Epoch 16 Batch 0 Loss 0.0271\n",
      "Epoch 16 Batch 100 Loss 0.0251\n",
      "Epoch 16 Batch 200 Loss 0.0569\n",
      "Epoch 16 Batch 300 Loss 0.0734\n",
      "Epoch 16 Batch 400 Loss 0.0865\n",
      "Epoch 16 Batch 500 Loss 0.0370\n",
      "Epoch 16 Batch 600 Loss 0.0651\n",
      "Epoch 16 Loss 0.0533\n",
      "Time taken for 1 epoch: 40.487908124923706 sec\n",
      "\n",
      "Epoch 17 Batch 0 Loss 0.0245\n",
      "Epoch 17 Batch 100 Loss 0.0307\n",
      "Epoch 17 Batch 200 Loss 0.0255\n",
      "Epoch 17 Batch 300 Loss 0.0465\n",
      "Epoch 17 Batch 400 Loss 0.0718\n",
      "Epoch 17 Batch 500 Loss 0.0402\n",
      "Epoch 17 Batch 600 Loss 0.0602\n",
      "Epoch 17 Loss 0.0489\n",
      "Time taken for 1 epoch: 39.62802314758301 sec\n",
      "\n",
      "Epoch 18 Batch 0 Loss 0.0471\n",
      "Epoch 18 Batch 100 Loss 0.0248\n",
      "Epoch 18 Batch 200 Loss 0.0344\n",
      "Epoch 18 Batch 300 Loss 0.0969\n",
      "Epoch 18 Batch 400 Loss 0.0350\n",
      "Epoch 18 Batch 500 Loss 0.0672\n",
      "Epoch 18 Batch 600 Loss 0.0278\n",
      "Epoch 18 Loss 0.0471\n",
      "Time taken for 1 epoch: 40.70291781425476 sec\n",
      "\n",
      "Epoch 19 Batch 0 Loss 0.0286\n",
      "Epoch 19 Batch 100 Loss 0.0373\n",
      "Epoch 19 Batch 200 Loss 0.0418\n",
      "Epoch 19 Batch 300 Loss 0.0355\n",
      "Epoch 19 Batch 400 Loss 0.0439\n",
      "Epoch 19 Batch 500 Loss 0.0652\n",
      "Epoch 19 Batch 600 Loss 0.0442\n",
      "Epoch 19 Loss 0.0490\n",
      "Time taken for 1 epoch: 38.49551510810852 sec\n",
      "\n",
      "Epoch 20 Batch 0 Loss 0.0123\n",
      "Epoch 20 Batch 100 Loss 0.0463\n",
      "Epoch 20 Batch 200 Loss 0.0237\n",
      "Epoch 20 Batch 300 Loss 0.0379\n",
      "Epoch 20 Batch 400 Loss 0.0574\n",
      "Epoch 20 Batch 500 Loss 0.0751\n",
      "Epoch 20 Batch 600 Loss 0.0544\n",
      "Epoch 20 Loss 0.0471\n",
      "Time taken for 1 epoch: 40.805777072906494 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for(batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1, batch, batch_loss.numpy()))\n",
    "            \n",
    "    # Saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch: {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    print(type(max_length_inp))\n",
    "    print(type(max_length_targ))\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "    \n",
    "   #     lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    \n",
    "   # lang_tokenizer.fit_on_texts(lang)\n",
    "    \n",
    "   # tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    \n",
    "   # tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "    \n",
    "   # return tensor, lang_tokenizer\n",
    "    sentence = tf.keras.preprocessing.text.text_to_word_sequence(sentence)\n",
    "    \n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_seqences(inputs,\n",
    "                                                          maxlen = max_length_inp,\n",
    "                                                          padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "    \n",
    "        # For storing attention plots later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        preicted_id = tf.argmax(predictions[0]).numpy()\n",
    "   \n",
    "        result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "        if(targ_lang.index_word[predicted_id] == '<end>'):\n",
    "            return result, sentence, attention_plot\n",
    "        \n",
    "        # The predicted ID is fed back into the model\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for plotting the attention weights:\n",
    "\n",
    "def plot_attention(attention, sentence, preicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontsize = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "    print('Input: %s'.format(sentence))\n",
    "    print('Predicted Translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "<class 'int'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-2d79cbbb75a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'hace mucho frio aqui.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-80-88140ea7e20e>\u001b[0m in \u001b[0;36mtranslate\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Input: %s'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Predicted Translation: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-78-f20f881bab71>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_to_word_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0minp_lang\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     inputs = tf.keras.preprocessing.sequence.pad_seqences(inputs,\n\u001b[0;32m     19\u001b[0m                                                           \u001b[0mmaxlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_length_inp\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "translate('hace mucho frio aqui.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'esta es mi vida.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(u'¿Todavia esta en casa?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-fun]",
   "language": "python",
   "name": "conda-env-.conda-fun-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
