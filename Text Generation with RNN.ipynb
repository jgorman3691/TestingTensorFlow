{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "conda-env-.conda-fun-py",
   "display_name": "Python [conda env:.conda-fun] *"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'http://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length of text: 99993 characters.\n"
     ]
    }
   ],
   "source": [
    "print('Length of text: {} characters.'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "That, poor contempt, or claim'd thou slept so faithful,\nI may contrive our father; and, in their defeated queen,\nHer flesh broke me and puttance of expedition house,\nAnd in that same that ever I lament this stomach,\nAnd he, nor Butly and my fury, kno\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of unique characters: 62\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print('Number of unique characters: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\n  '\\n':   0,\n  ' ' :   1,\n  '!' :   2,\n  \"'\" :   3,\n  ',' :   4,\n  '-' :   5,\n  '.' :   6,\n  ':' :   7,\n  ';' :   8,\n  '?' :   9,\n  'A' :  10,\n  'B' :  11,\n  'C' :  12,\n  'D' :  13,\n  'E' :  14,\n  'F' :  15,\n  'G' :  16,\n  'H' :  17,\n  'I' :  18,\n  'J' :  19,\n   ...\n}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('   ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "'That, poor co' ---- characters mapped to int ---- > [29 43 36 55  4  1 51 50 50 53  1 38 50]\n"
     ]
    }
   ],
   "source": [
    "print('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "T\nh\na\nt\n,\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length + 1)\n",
    "\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"That, poor contempt, or claim'd thou slept so faithful,\\nI may contrive our father; and, in their defe\"\n'ated queen,\\nHer flesh broke me and puttance of expedition house,\\nAnd in that same that ever I lament '\n'this stomach,\\nAnd he, nor Butly and my fury, knowing everything\\nGrew daily ever, his great strength a'\n\"nd thought\\nThe bright buds of mine own.\\n\\nBIONDELLO:\\nMarry, that it may not pray their patience.'\\n\\nKIN\"\n'G LEAR:\\nThe instant common maid, as we may less be\\na brave gentleman and joiner: he that finds us wit'\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "for item in sequences.take(5):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input data:  \"That, poor contempt, or claim'd thou slept so faithful,\\nI may contrive our father; and, in their def\"\nTarget data:  \"hat, poor contempt, or claim'd thou slept so faithful,\\nI may contrive our father; and, in their defe\"\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print('Target data: ', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Step    0\n  input: 29 ('T')\n  exptected output: 43  ('h')\nStep    1\n  input: 43 ('h')\n  exptected output: 36  ('a')\nStep    2\n  input: 36 ('a')\n  exptected output: 55  ('t')\nStep    3\n  input: 55 ('t')\n  exptected output: 4  (',')\nStep    4\n  input: 4 (',')\n  exptected output: 1  (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  exptected output: {}  ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "embedding_dim = 256\n",
    "\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, batch_input_shape=[batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units,\n",
    "                            return_sequences=True,\n",
    "                            stateful=True,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(64, 100, 62) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (64, None, 256)           15872     \n_________________________________________________________________\nlstm_2 (LSTM)                (64, None, 1024)          5246976   \n_________________________________________________________________\ndense_2 (Dense)              (64, None, 62)            63550     \n=================================================================\nTotal params: 5,326,398\nTrainable params: 5,326,398\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([36, 23,  4, 33, 28, 14, 27, 46,  8, 46, 43, 39,  1, 58, 44, 61, 54,\n",
       "       31,  3,  8, 18, 17, 52, 38, 19, 56, 29, 34, 53, 34, 33, 12, 30, 58,\n",
       "       59, 13, 28, 42, 28,  5, 52, 13, 19,  7, 44, 36,  4,  6, 53, 42, 56,\n",
       "       45, 34, 33,  1,  4, 50, 23, 16, 61, 43, 57, 29, 35, 11, 30, 40, 21,\n",
       "       38, 57, 46, 12, 24, 47,  5, 31, 48, 52, 17,  5, 29, 46, 48,  4, 40,\n",
       "       47, 48, 48,  1, 19, 35, 52, 10, 31, 41, 59, 15, 55,  1,  2],\n",
       "      dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Input: \n \"ens and desired all\\nFor what with mountain blasting phrase or wonder\\nMakes lobes that stabb'd thy co\"\n\nNext character predictions: \n \"aN,XSERk;khd wizsV';IHqcJuTYrYXCUwxDSgS-qDJ:ia,.rgujYX ,oNGzhvTZBUeLcvkCOl-VmqH-Tkm,elmm JZqAVfxFt !\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(''.join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next character predictions: \\n\", repr(''.join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Prediction shape:  (64, 100, 62)  # (batch_size, sequence_length, vocab_size)\nScalar Loss:  4.1270747\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Scalar Loss: \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/30\n",
      "15/15 [==============================] - 8s 530ms/step - loss: 3.4697\n",
      "Epoch 2/30\n",
      "15/15 [==============================] - 8s 513ms/step - loss: 3.2028\n",
      "Epoch 3/30\n",
      "15/15 [==============================] - 8s 566ms/step - loss: 2.9923\n",
      "Epoch 4/30\n",
      "15/15 [==============================] - 8s 514ms/step - loss: 2.6418\n",
      "Epoch 5/30\n",
      "15/15 [==============================] - 8s 510ms/step - loss: 2.4580\n",
      "Epoch 6/30\n",
      "15/15 [==============================] - 8s 561ms/step - loss: 2.3577\n",
      "Epoch 7/30\n",
      "15/15 [==============================] - 8s 561ms/step - loss: 2.2804\n",
      "Epoch 8/30\n",
      "15/15 [==============================] - 8s 563ms/step - loss: 2.2137\n",
      "Epoch 9/30\n",
      "15/15 [==============================] - 8s 566ms/step - loss: 2.1562\n",
      "Epoch 10/30\n",
      "15/15 [==============================] - 8s 563ms/step - loss: 2.0970\n",
      "Epoch 11/30\n",
      "15/15 [==============================] - 8s 526ms/step - loss: 2.0416\n",
      "Epoch 12/30\n",
      "15/15 [==============================] - 9s 567ms/step - loss: 1.9898\n",
      "Epoch 13/30\n",
      "15/15 [==============================] - 8s 562ms/step - loss: 1.9452\n",
      "Epoch 14/30\n",
      "15/15 [==============================] - 8s 560ms/step - loss: 1.8966\n",
      "Epoch 15/30\n",
      "15/15 [==============================] - 8s 562ms/step - loss: 1.8464\n",
      "Epoch 16/30\n",
      "15/15 [==============================] - 8s 563ms/step - loss: 1.8046\n",
      "Epoch 17/30\n",
      "15/15 [==============================] - 8s 559ms/step - loss: 1.7596\n",
      "Epoch 18/30\n",
      "15/15 [==============================] - 8s 562ms/step - loss: 1.7156\n",
      "Epoch 19/30\n",
      "15/15 [==============================] - 8s 536ms/step - loss: 1.6752\n",
      "Epoch 20/30\n",
      "15/15 [==============================] - 8s 513ms/step - loss: 1.6277\n",
      "Epoch 21/30\n",
      "15/15 [==============================] - 8s 559ms/step - loss: 1.5830\n",
      "Epoch 22/30\n",
      "15/15 [==============================] - 8s 561ms/step - loss: 1.5371\n",
      "Epoch 23/30\n",
      "15/15 [==============================] - 8s 564ms/step - loss: 1.4904\n",
      "Epoch 24/30\n",
      "15/15 [==============================] - 8s 510ms/step - loss: 1.4447\n",
      "Epoch 25/30\n",
      "15/15 [==============================] - 8s 564ms/step - loss: 1.3934\n",
      "Epoch 26/30\n",
      "15/15 [==============================] - 8s 561ms/step - loss: 1.3400\n",
      "Epoch 27/30\n",
      "15/15 [==============================] - 8s 561ms/step - loss: 1.2812\n",
      "Epoch 28/30\n",
      "15/15 [==============================] - 8s 562ms/step - loss: 1.2226\n",
      "Epoch 29/30\n",
      "15/15 [==============================] - 8s 564ms/step - loss: 1.1612\n",
      "Epoch 30/30\n",
      "15/15 [==============================] - 8s 561ms/step - loss: 1.0964\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'./training_checkpoints\\\\ckpt_30'"
      ]
     },
     "metadata": {},
     "execution_count": 76
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (1, None, 256)            15872     \n_________________________________________________________________\nlstm_3 (LSTM)                (1, None, 1024)           5246976   \n_________________________________________________________________\ndense_3 (Dense)              (1, None, 62)             63550     \n=================================================================\nTotal params: 5,326,398\nTrainable params: 5,326,398\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    \n",
    "    num_generate = 1000\n",
    "\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    temperature = 1.0\n",
    "\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        predictions /= temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ROMEO: BLICDEUDu-SNLASMLGQIVIIUTC-UPSSBIX IPUSSANIA:\nthere's\nmocheref shall be with lefare,\nAnd you blooked of by their death-do.\n\nMARK ANTONY:\nFo- to do.\n\nCHENMINA:\nEven ale my hood uppean beteem's ore: he's cause not and bewam\nRon thus he not old were shall adated.\n\nMELUT:\nShall may we dray? To-pack to so hear my good in.\n\nLADD HANK:\nNow.\n\nHINE PENNY\nMoster'' timl father, now thet we would have come did the wirt, be manigld.\n\nHOENE:\nThou mast fercomen:\nThin think of their vestence would not to a man learm\nTike for he's to our than morise fay not that with bearemoul\nTo am the rodam and diphink it our hose;\nFor not Hesh way not Tell and me then.\n\nMARK ANTONY:\nThe pails 'at some swaiff and shorous in the leament loke,\nThe trust for in sufficcimed to thet he thine in thy magre\nTo must of me cape of you have sweet here,\nAn one insurberaveingers of Listabuse.\nThis will come thee athicr my forthian to till known,\nGo love that this rust reloote to did wear in thy good loys,\nAmacestod and tome ano h\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    vocab_size=len(vocab),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inp)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.keras.losses.sparse_categorical_crossentropy(\n",
    "                target, predictions, from_logits=True))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1 Batch 0 Loss 4.126844882965088\n",
      "Epoch 1 Loss 3.2846\n",
      "Time taken for a single epoch: 20.90825390815735 sec\n",
      "\n",
      "Epoch 2 Batch 0 Loss 3.3290772438049316\n",
      "Epoch 2 Loss 3.1706\n",
      "Time taken for a single epoch: 19.96717882156372 sec\n",
      "\n",
      "Epoch 3 Batch 0 Loss 4.0030364990234375\n",
      "Epoch 3 Loss 3.0431\n",
      "Time taken for a single epoch: 19.212836265563965 sec\n",
      "\n",
      "Epoch 4 Batch 0 Loss 2.999776840209961\n",
      "Epoch 4 Loss 2.6755\n",
      "Time taken for a single epoch: 19.195477962493896 sec\n",
      "\n",
      "Epoch 5 Batch 0 Loss 2.68159556388855\n",
      "Epoch 5 Loss 2.4814\n",
      "Time taken for a single epoch: 18.657591104507446 sec\n",
      "\n",
      "Epoch 6 Batch 0 Loss 2.466998338699341\n",
      "Epoch 6 Loss 2.3641\n",
      "Time taken for a single epoch: 18.807147979736328 sec\n",
      "\n",
      "Epoch 7 Batch 0 Loss 2.4043960571289062\n",
      "Epoch 7 Loss 2.3069\n",
      "Time taken for a single epoch: 19.169296741485596 sec\n",
      "\n",
      "Epoch 8 Batch 0 Loss 2.320762872695923\n",
      "Epoch 8 Loss 2.2867\n",
      "Time taken for a single epoch: 19.17879056930542 sec\n",
      "\n",
      "Epoch 9 Batch 0 Loss 2.2612082958221436\n",
      "Epoch 9 Loss 2.2053\n",
      "Time taken for a single epoch: 20.100284099578857 sec\n",
      "\n",
      "Epoch 10 Batch 0 Loss 2.2121665477752686\n",
      "Epoch 10 Loss 2.1647\n",
      "Time taken for a single epoch: 19.659367084503174 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    model.reset_states()\n",
    "\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        loss = train_step(inp, target)\n",
    "\n",
    "        if batch_n % 100 == 0:\n",
    "            template = 'Epoch {} Batch {} Loss {}'\n",
    "            print(template.format(epoch+1, batch_n, loss))\n",
    "\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print('Time taken for a single epoch: {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  }
 ]
}